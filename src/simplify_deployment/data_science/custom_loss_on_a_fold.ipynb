{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn \n",
    "from sklearn.datasets import make_regression\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from typing import Tuple\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import plotly.express as px\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import root_mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 8\n",
    "data_folder = Path(\n",
    "    \"/home/thomas/repos/simplify_deployment/data/data_science/50_gen_s1/folds_converted_with_genome\",\n",
    ")\n",
    "X_train = pd.read_parquet(data_folder/f\"X_train_fold_{fold}.parquet\")\n",
    "y_train = pd.read_parquet(data_folder/f\"y_train_fold_{fold}.parquet\").squeeze()\n",
    "X_test = pd.read_parquet(data_folder/f\"X_test_fold_{fold}.parquet\")\n",
    "y_test = pd.read_parquet(data_folder/f\"y_test_fold_{fold}.parquet\").squeeze()\n",
    "n_features = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard linear regression\n",
    "Let's start by fitting a standard linear regression and plotting the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_model = LinearRegression()\n",
    "standard_model.fit(X_train, y_train)\n",
    "prediction_df = pd.DataFrame(\n",
    "    {\n",
    "        \"y_true\": y_test,\n",
    "        \"y_pred_standard_model\": standard_model.predict(X_test)\n",
    "    },\n",
    "    index = y_test.index\n",
    ")\n",
    "prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df_molten = prediction_df.melt(ignore_index=False)\n",
    "fig = px.line(\n",
    "    prediction_df_molten,\n",
    "    x = prediction_df_molten.index,\n",
    "    y = \"value\",\n",
    "    color = \"variable\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom loss model\n",
    "This time we will fit a linear model with our custom loss function. It should perform less well in regards to RMSE as this it not what it optimizes for.\n",
    "We should see less max errors, or less time above threshold, ... depending on what we prioritize in the loss function.\n",
    "\n",
    "For the optimizer to work well, it is best to standardscale the data. This brings some added complexity in the code, but is necessary to speed up convergence.\n",
    "\n",
    "Let's starts by setting up some classes we will need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_features: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(\n",
    "            in_features=n_features,\n",
    "            out_features=1,\n",
    "        )  # Just 1 fully connected layer without activation, i.e. a linear regression.\n",
    "\n",
    "    def forward(\n",
    "        self,  \n",
    "        X: torch.Tensor,  \n",
    "    ) -> torch.Tensor:\n",
    "        y = self.fc1(X)\n",
    "        return y.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            threshold: float = 0.685,\n",
    "            weight_max_error: float = 1,\n",
    "            weight_percentage_above_threshold:float = 1,\n",
    "            weight_wrong_sign: float = 1,\n",
    "            sigmoid_steepness: float = 100,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.steepness = sigmoid_steepness\n",
    "        self.threshold = threshold\n",
    "        # Normalize weights and assign them\n",
    "        sum_weights = (\n",
    "            weight_max_error\n",
    "            + weight_percentage_above_threshold\n",
    "            + weight_wrong_sign\n",
    "        )\n",
    "        self.weight_max_error = (\n",
    "            weight_max_error / sum_weights\n",
    "        )\n",
    "        self.weight_percentage_above_threshold = (\n",
    "            weight_percentage_above_threshold / sum_weights\n",
    "        )\n",
    "        self.weight_wrong_sign = (\n",
    "            weight_wrong_sign / sum_weights\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            inputs: torch.Tensor, \n",
    "            targets: torch.Tensor,\n",
    "        ) -> torch.Tensor:\n",
    "\n",
    "        residuals = targets - inputs\n",
    "        # Maximum abs error\n",
    "        max_error = residuals.abs().max()\n",
    "\n",
    "        # Percentage of time above threshold value\n",
    "        percentage_of_time_above_x = (\n",
    "            1/(1+torch.e**(-self.steepness*(residuals.abs()-self.threshold)))\n",
    "        ).mean()\n",
    "\n",
    "        # Percentage of time wrong sign\n",
    "        loss_percentage_of_time_wrong_sign = (\n",
    "            1/(1+torch.e**(-self.steepness*(inputs*targets)))\n",
    "        ).mean()\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = (\n",
    "            self.weight_max_error * max_error\n",
    "            + self.weight_percentage_above_threshold * percentage_of_time_above_x\n",
    "            + self.weight_wrong_sign * loss_percentage_of_time_wrong_sign\n",
    "        )\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMSELoss(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            inputs: torch.Tensor, \n",
    "            targets: torch.Tensor,\n",
    "        ) -> torch.Tensor:\n",
    "\n",
    "        residuals = targets - inputs\n",
    "        mse = (residuals**2).mean()\n",
    "        return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            X: torch.Tensor,\n",
    "            y: torch.Tensor,\n",
    "        ) -> None:\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(\n",
    "            self\n",
    "    ) -> int:\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(\n",
    "            self,\n",
    "            idx: int,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        X_item = self.X[idx,:]\n",
    "        y_item = self.y[idx]\n",
    "        return X_item, y_item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we created our custom loss and model, we need to start prepping the data.\n",
    "\n",
    "- Step 1: Standardscale X and y\n",
    "- Step 2: Convert data to Tensors\n",
    "- Step 3: Optimization loop on Tensors\n",
    "- Step 4: Convert Tensors back and undo scaling so we can compare with previous results\n",
    "\n",
    "As a lot of steps here might introduce bugs, a sanity check will be no luxury. We will start with a custom loop that also implements MSE loss. \n",
    "This should give the same result as the Sklearn implementation. If this is the case, we passed our sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaler = StandardScaler().fit(X_train)\n",
    "y_scaler = StandardScaler().fit(y_train.to_frame())\n",
    "\n",
    "X_train_scaled = torch.Tensor(\n",
    "    X_scaler.transform(X_train),\n",
    ").float()\n",
    "\n",
    "X_test_scaled = torch.Tensor(\n",
    "    X_scaler.transform(X_test),\n",
    ").float()\n",
    "\n",
    "y_train_scaled = torch.Tensor(\n",
    "    y_scaler.transform(y_train.to_frame()).squeeze(), # Make target tensor unidimensional\n",
    ").float()\n",
    "\n",
    "y_test_scaled = torch.Tensor(\n",
    "    y_scaler.transform(y_test.to_frame()).squeeze(),\n",
    ").float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard MSE loss, sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "lr = 1e-4\n",
    "batch_size = 10\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    CustomDataset(X_train_scaled,y_train_scaled),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "model = Model(\n",
    "    n_features=n_features,\n",
    ")\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr = lr,\n",
    ")\n",
    "criterion = CustomMSELoss()\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for i,(X_batch, y_batch) in enumerate(dataloader):\n",
    "        prediction = model(X_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(prediction, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        average_loss = epoch_loss/(i+1)\n",
    "    print(f\"Average epoch loss: {average_loss}\")\n",
    "    print(f\"Epoch {epoch} done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df[\"y_pred_sanity_check\"] = y_scaler.inverse_transform(\n",
    "    model(X_test_scaled).detach().numpy().reshape(-1,1),\n",
    ")\n",
    "prediction_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df_molten = prediction_df.melt(ignore_index=False)\n",
    "fig = px.line(\n",
    "    prediction_df_molten,\n",
    "    x = prediction_df_molten.index,\n",
    "    y = \"value\",\n",
    "    color = \"variable\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the sanity check is still not totally equal to the sklearn model. This might be due to the way the model is trained. \n",
    "In sklearn we optimize the RMSE over all in one go. With our torch loop we work per batch and minimize the rmse per batch. We see that in practice the predictions are however not too far off. In theory model with an optimal rmse per batch will also have a global optimal RMSE. If we really really want to check this, we can give it a lot more epochs.\n",
    "\n",
    "Out of interest the RMSE values for both on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_standard_model = root_mean_squared_error(\n",
    "    prediction_df[\"y_true\"],\n",
    "    prediction_df[\"y_pred_standard_model\"],\n",
    ")\n",
    "rmse_torch_model = root_mean_squared_error(\n",
    "    prediction_df[\"y_true\"],\n",
    "    prediction_df[\"y_pred_sanity_check\"],\n",
    ")\n",
    "print(f\"RMSE of sklearn model is {rmse_standard_model}\")\n",
    "print(f\"RMSE of torch model is {rmse_torch_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom loss model\n",
    "Now what you have been actually waiting for, the model with the custom loss function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "lr = 1e-4\n",
    "batch_size = 10\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    CustomDataset(X_train_scaled,y_train_scaled),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "model_custom_loss = Model(\n",
    "    n_features=n_features,\n",
    ")\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr = lr,\n",
    ")\n",
    "# Part of the loss function will be how much time the absolute value of the residuals is above a threshold, 117 MW if i'm not mistaken.\n",
    "# However we converted our y values in z-scores as this helps the optimizer a lot. So we will too for this threshold.\n",
    "# A mistake to avoid is to just convert this threshold in the z score of the threshold. \n",
    "# E.g. imagine: sigma = 10 (standarddeviation), mu=5 (mean). \n",
    "# Problem: we have 2 values, say 20 & 15 and we want to check if their residuals are above or equal to a threshold of 5\n",
    "# In non z-score land this is easy: 20-15 = 5, so equal to the threshold.\n",
    "# In z-score land we got (20-5)/10 - (15-5)/10 = 0.5 -> This is not the z-score of 5! (which is zero) \n",
    "# So make sure to not convert the threshold to its z-score.\n",
    "# But then how do we convert it? With a small bit of calculus we can show that the threshold in z-score land is the normal threshold/sigma\n",
    "# E.g. 5/10 would be 0.5 in z-score land. Which is indeed correct for our example.\n",
    "threshold = 117\n",
    "converted_threshold = threshold / np.std(y_train)\n",
    "criterion = CustomLoss(\n",
    "    threshold=converted_threshold,\n",
    "    weight_wrong_sign=0,\n",
    "    weight_max_error=1,\n",
    "    weight_percentage_above_threshold=0,\n",
    ") # This is our custom loss. You can give random weights to the different factors of the loss function. \n",
    "# Or feel free to implement something yourself in the loss.\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for i,(X_batch, y_batch) in enumerate(dataloader):\n",
    "        prediction = model_custom_loss(X_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(prediction, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        average_loss = epoch_loss/(i+1)\n",
    "    print(f\"Average epoch loss: {average_loss}\")\n",
    "    print(f\"Epoch {epoch} done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df[\"y_pred_custom_loss\"] = y_scaler.inverse_transform(\n",
    "    model_custom_loss(X_test_scaled).detach().numpy().reshape(-1,1),\n",
    ")\n",
    "prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_rmse = root_mean_squared_error(\n",
    "    prediction_df[\"y_true\"],\n",
    "    prediction_df[\"y_pred_custom_loss\"],\n",
    ")\n",
    "print(f\"Rmse of custom model is {custom_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df_molten = prediction_df.melt(ignore_index=False)\n",
    "fig = px.line(\n",
    "    prediction_df_molten,\n",
    "    x = prediction_df_molten.index,\n",
    "    y = \"value\",\n",
    "    color = \"variable\",\n",
    ")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simplify-deployment-2FnGvFJr-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
