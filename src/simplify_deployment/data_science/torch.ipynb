{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from typing import Tuple\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature1_first_derivative</th>\n",
       "      <th>feature1_second_derivative</th>\n",
       "      <th>feature2</th>\n",
       "      <th>random</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.838443</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-98.675476</td>\n",
       "      <td>-54.966846</td>\n",
       "      <td>125.352361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.923841</td>\n",
       "      <td>18.085398</td>\n",
       "      <td>18.085398</td>\n",
       "      <td>-18.326730</td>\n",
       "      <td>-0.767651</td>\n",
       "      <td>262.028397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90.801856</td>\n",
       "      <td>61.878015</td>\n",
       "      <td>43.792617</td>\n",
       "      <td>-26.714492</td>\n",
       "      <td>-72.100158</td>\n",
       "      <td>705.500567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-31.657460</td>\n",
       "      <td>-122.459316</td>\n",
       "      <td>-184.337331</td>\n",
       "      <td>-49.044406</td>\n",
       "      <td>47.481838</td>\n",
       "      <td>-1667.009776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>86.360293</td>\n",
       "      <td>118.017753</td>\n",
       "      <td>240.477068</td>\n",
       "      <td>-54.990580</td>\n",
       "      <td>-15.658911</td>\n",
       "      <td>2270.103902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>-25.671062</td>\n",
       "      <td>-123.657465</td>\n",
       "      <td>-318.054069</td>\n",
       "      <td>94.062701</td>\n",
       "      <td>50.809121</td>\n",
       "      <td>-2737.755707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>-20.158123</td>\n",
       "      <td>5.512939</td>\n",
       "      <td>129.170404</td>\n",
       "      <td>23.052854</td>\n",
       "      <td>42.407890</td>\n",
       "      <td>862.362543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>85.923437</td>\n",
       "      <td>106.081560</td>\n",
       "      <td>100.568621</td>\n",
       "      <td>16.140472</td>\n",
       "      <td>22.884506</td>\n",
       "      <td>1182.931431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>-46.886607</td>\n",
       "      <td>-132.810043</td>\n",
       "      <td>-238.891603</td>\n",
       "      <td>-57.017945</td>\n",
       "      <td>-4.506971</td>\n",
       "      <td>-2102.426623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>66.980179</td>\n",
       "      <td>113.866786</td>\n",
       "      <td>246.676829</td>\n",
       "      <td>44.648403</td>\n",
       "      <td>91.194332</td>\n",
       "      <td>2162.650119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       feature1  feature1_first_derivative  feature1_second_derivative  \\\n",
       "0     10.838443                   0.000000                    0.000000   \n",
       "1     28.923841                  18.085398                   18.085398   \n",
       "2     90.801856                  61.878015                   43.792617   \n",
       "3    -31.657460                -122.459316                 -184.337331   \n",
       "4     86.360293                 118.017753                  240.477068   \n",
       "...         ...                        ...                         ...   \n",
       "9995 -25.671062                -123.657465                 -318.054069   \n",
       "9996 -20.158123                   5.512939                  129.170404   \n",
       "9997  85.923437                 106.081560                  100.568621   \n",
       "9998 -46.886607                -132.810043                 -238.891603   \n",
       "9999  66.980179                 113.866786                  246.676829   \n",
       "\n",
       "       feature2     random       target  \n",
       "0    -98.675476 -54.966846   125.352361  \n",
       "1    -18.326730  -0.767651   262.028397  \n",
       "2    -26.714492 -72.100158   705.500567  \n",
       "3    -49.044406  47.481838 -1667.009776  \n",
       "4    -54.990580 -15.658911  2270.103902  \n",
       "...         ...        ...          ...  \n",
       "9995  94.062701  50.809121 -2737.755707  \n",
       "9996  23.052854  42.407890   862.362543  \n",
       "9997  16.140472  22.884506  1182.931431  \n",
       "9998 -57.017945  -4.506971 -2102.426623  \n",
       "9999  44.648403  91.194332  2162.650119  \n",
       "\n",
       "[10000 rows x 6 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 10000\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"feature1\": np.random.uniform(-100,100,size = n),\n",
    "    }    \n",
    ")\n",
    "df[\"feature1_first_derivative\"]= df[\"feature1\"].diff(1).fillna(0)\n",
    "df[\"feature1_second_derivative\"]= df[\"feature1_first_derivative\"].diff(1).fillna(0)\n",
    "df[\"feature2\"]= np.random.uniform(-100,100,size = n)\n",
    "df[\"random\"] = np.random.uniform(-100,100,size = n)\n",
    "df[\"target\"] = 2*df[\"feature1\"] + 3*df[\"feature1_first_derivative\"] + 7*df[\"feature1_second_derivative\"]- df[\"feature2\"]  + 5\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature1_first_derivative</th>\n",
       "      <th>feature1_second_derivative</th>\n",
       "      <th>feature2</th>\n",
       "      <th>random</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.184060</td>\n",
       "      <td>-0.000069</td>\n",
       "      <td>-0.000081</td>\n",
       "      <td>-1.703983</td>\n",
       "      <td>-0.951038</td>\n",
       "      <td>0.094832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.497663</td>\n",
       "      <td>0.221767</td>\n",
       "      <td>0.128239</td>\n",
       "      <td>-0.317938</td>\n",
       "      <td>-0.010372</td>\n",
       "      <td>0.202921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.570637</td>\n",
       "      <td>0.758930</td>\n",
       "      <td>0.310637</td>\n",
       "      <td>-0.462630</td>\n",
       "      <td>-1.248399</td>\n",
       "      <td>0.553637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.552825</td>\n",
       "      <td>-1.502160</td>\n",
       "      <td>-1.307994</td>\n",
       "      <td>-0.847829</td>\n",
       "      <td>0.827032</td>\n",
       "      <td>-1.322642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.493619</td>\n",
       "      <td>1.447542</td>\n",
       "      <td>1.706156</td>\n",
       "      <td>-0.950403</td>\n",
       "      <td>-0.268821</td>\n",
       "      <td>1.790990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>-0.449020</td>\n",
       "      <td>-1.516857</td>\n",
       "      <td>-2.256744</td>\n",
       "      <td>1.620819</td>\n",
       "      <td>0.884780</td>\n",
       "      <td>-2.169433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>-0.353425</td>\n",
       "      <td>0.067553</td>\n",
       "      <td>0.916411</td>\n",
       "      <td>0.395874</td>\n",
       "      <td>0.738970</td>\n",
       "      <td>0.677690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>1.486044</td>\n",
       "      <td>1.301132</td>\n",
       "      <td>0.713475</td>\n",
       "      <td>0.276633</td>\n",
       "      <td>0.400128</td>\n",
       "      <td>0.931209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>-0.816901</td>\n",
       "      <td>-1.629123</td>\n",
       "      <td>-1.695069</td>\n",
       "      <td>-0.985376</td>\n",
       "      <td>-0.075271</td>\n",
       "      <td>-1.666988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>1.157565</td>\n",
       "      <td>1.396626</td>\n",
       "      <td>1.750145</td>\n",
       "      <td>0.768405</td>\n",
       "      <td>1.585694</td>\n",
       "      <td>1.706011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature1  feature1_first_derivative  feature1_second_derivative  \\\n",
       "0     0.184060                  -0.000069                   -0.000081   \n",
       "1     0.497663                   0.221767                    0.128239   \n",
       "2     1.570637                   0.758930                    0.310637   \n",
       "3    -0.552825                  -1.502160                   -1.307994   \n",
       "4     1.493619                   1.447542                    1.706156   \n",
       "...        ...                        ...                         ...   \n",
       "9995 -0.449020                  -1.516857                   -2.256744   \n",
       "9996 -0.353425                   0.067553                    0.916411   \n",
       "9997  1.486044                   1.301132                    0.713475   \n",
       "9998 -0.816901                  -1.629123                   -1.695069   \n",
       "9999  1.157565                   1.396626                    1.750145   \n",
       "\n",
       "      feature2    random    target  \n",
       "0    -1.703983 -0.951038  0.094832  \n",
       "1    -0.317938 -0.010372  0.202921  \n",
       "2    -0.462630 -1.248399  0.553637  \n",
       "3    -0.847829  0.827032 -1.322642  \n",
       "4    -0.950403 -0.268821  1.790990  \n",
       "...        ...       ...       ...  \n",
       "9995  1.620819  0.884780 -2.169433  \n",
       "9996  0.395874  0.738970  0.677690  \n",
       "9997  0.276633  0.400128  0.931209  \n",
       "9998 -0.985376 -0.075271 -1.666988  \n",
       "9999  0.768405  1.585694  1.706011  \n",
       "\n",
       "[10000 rows x 6 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler().set_output(transform = \"pandas\")\n",
    "df = scaler.fit_transform(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplifyModel(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_inputs: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(n_inputs,1)\n",
    "        nn.init.kaiming_uniform_(self.fc1.weight)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        x: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        pred = self.fc1(x)\n",
    "        return pred\n",
    "       \n",
    "class ChunkDataset(Dataset):\n",
    "    def __init__(\n",
    "            self, \n",
    "            X_chunk: pd.DataFrame,\n",
    "            y_chunk: pd.Series,\n",
    "    ) -> None:\n",
    "        self.X_chunk = X_chunk\n",
    "        self.y_chunk = y_chunk\n",
    "\n",
    "    def __len__(\n",
    "            self,\n",
    "    ) -> int:\n",
    "        return self.X_chunk.shape[0]\n",
    "        \n",
    "    def __getitem__(\n",
    "            self, \n",
    "            index\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        X = torch.Tensor(self.X_chunk.iloc[index,:].values).float()\n",
    "\n",
    "        y = torch.Tensor([self.y_chunk.iloc[index]]).float()\n",
    "        return X,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.0220, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.5805, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6198, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7144, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4201, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1498, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6036, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1147, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0788, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0853, grad_fn=<MseLossBackward0>)\n",
      "Epoch 0 done.\n",
      "tensor(0.0600, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0537, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0338, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0232, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0148, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0144, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0184, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0121, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0082, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0112, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1 done.\n",
      "tensor(0.0053, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0098, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0047, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0034, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0039, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0070, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0042, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0059, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0070, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2 done.\n",
      "tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0052, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0043, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0033, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0035, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3 done.\n",
      "tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4 done.\n",
      "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.6895e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5 done.\n",
      "tensor(3.9241e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.9017e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.2580e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.5014e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.5904e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.7465e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.7327e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1334e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.4868e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.8644e-06, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6 done.\n",
      "tensor(1.4465e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.2959e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.4347e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.2349e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.7736e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.7487e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(9.5325e-08, grad_fn=<MseLossBackward0>)\n",
      "tensor(9.2697e-08, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.7981e-08, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.9080e-08, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7 done.\n",
      "tensor(6.0715e-09, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.2476e-09, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3949e-09, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.2072e-10, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.8269e-10, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.4315e-10, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.1046e-10, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4465e-10, grad_fn=<MseLossBackward0>)\n",
      "tensor(9.1037e-11, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.8912e-11, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8 done.\n",
      "tensor(6.0108e-11, grad_fn=<MseLossBackward0>)\n",
      "tensor(9.0832e-11, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.9622e-11, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.9708e-11, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.7951e-11, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.0267e-11, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.8981e-11, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.5767e-11, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.4898e-11, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.9340e-11, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9 done.\n",
      "tensor(3.9728e-11, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.2037e-11, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.9898e-11, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.0191e-11, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.7084e-11, grad_fn=<MseLossBackward0>)\n",
      "tensor(9.4233e-11, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1607e-10, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.8243e-11, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.7638e-11, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.9841e-11, grad_fn=<MseLossBackward0>)\n",
      "Epoch 10 done.\n",
      "tensor(3.2096e-11, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.1360e-11, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.4349e-11, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.2887e-11, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.4410e-11, grad_fn=<MseLossBackward0>)\n",
      "tensor(9.7694e-11, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.5514e-10, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.2160e-11, grad_fn=<MseLossBackward0>)\n",
      "tensor(9.9357e-11, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.0950e-11, grad_fn=<MseLossBackward0>)\n",
      "Epoch 11 done.\n",
      "tensor(3.3520e-11, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0385e-10, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.3535e-11, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.4792e-11, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.4357e-11, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0848e-10, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.8119e-10, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.8065e-11, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.2898e-11, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2357e-10, grad_fn=<MseLossBackward0>)\n",
      "Epoch 12 done.\n",
      "tensor(9.0321e-11, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4647e-10, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.2647e-11, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.9426e-11, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.4806e-11, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.1400e-11, grad_fn=<MseLossBackward0>)\n",
      "tensor(9.1628e-11, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.3361e-11, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.1232e-10, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1653e-10, grad_fn=<MseLossBackward0>)\n",
      "Epoch 13 done.\n",
      "tensor(1.3987e-10, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3295e-10, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2281e-10, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.8420e-11, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.5284e-10, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.3941e-11, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.4914e-10, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.8798e-10, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0348e-09, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3299e-10, grad_fn=<MseLossBackward0>)\n",
      "Epoch 14 done.\n",
      "tensor(4.5080e-09, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0624e-08, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.3983e-09, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.9225e-08, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0781e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.8205e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2978e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.2451e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0872e-08, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0887e-09, grad_fn=<MseLossBackward0>)\n",
      "Epoch 15 done.\n",
      "tensor(3.1495e-10, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4118e-10, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.8755e-10, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.1737e-10, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.6938e-10, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.4649e-10, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2095e-10, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0887e-08, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.7041e-08, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.9259e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch 16 done.\n",
      "tensor(5.0919e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.8455e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.8035e-09, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.3294e-11, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.1521e-10, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.2475e-11, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3464e-10, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1354e-10, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.6949e-10, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.6704e-10, grad_fn=<MseLossBackward0>)\n",
      "Epoch 17 done.\n",
      "tensor(1.2477e-10, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3261e-10, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.9380e-10, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.0198e-10, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4073e-10, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.8351e-10, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.0642e-11, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.3490e-08, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2989e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.0942e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch 18 done.\n",
      "tensor(7.9679e-08, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.6205e-09, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.5095e-10, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.2526e-11, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.6045e-10, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.0845e-10, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2785e-10, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.5161e-09, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.7322e-08, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.3151e-07, grad_fn=<MseLossBackward0>)\n",
      "Epoch 19 done.\n"
     ]
    }
   ],
   "source": [
    "plt_dict = {\n",
    "    \"x\": [],\n",
    "    \"y\": [],\n",
    "}\n",
    "batch_size = 10\n",
    "epochs = 20\n",
    "model = SimplifyModel(\n",
    "    n_inputs=df.shape[1]-1,\n",
    ")\n",
    "dataset = ChunkDataset(\n",
    "    X_chunk=df.drop(columns = \"target\"),\n",
    "    y_chunk=df[\"target\"],\n",
    ")\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = Adam(\n",
    "    params = model.parameters(),\n",
    "    lr = 1/1000,\n",
    "    weight_decay=1e-5,\n",
    "\n",
    ")\n",
    "for epoch in range(epochs):\n",
    "    for i,(X, y_true) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X)\n",
    "        loss = criterion(\n",
    "            y_pred,\n",
    "            y_true,\n",
    "        )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 100 == 0:\n",
    "            print(loss)\n",
    "        plt_dict[\"x\"].append(i*(epoch+1))\n",
    "        plt_dict[\"y\"].append(loss.item())\n",
    "        \n",
    "    print(f\"Epoch {epoch} done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(plt_dict[\"x\"],plt_dict[\"y\"])\n",
    "ax.set_xlabel(\"iteration\")\n",
    "ax.set_ylabel(\"loss\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0005], requires_grad=True)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc1.bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0910,  0.1940,  0.7806, -0.0442,  0.0022]], requires_grad=True)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmModel(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=2,\n",
    "            hidden_size=8,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc1 = nn.Linear(8, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y_lstm, (hn, cn) = self.lstm(x)\n",
    "        # Use the output of the last time step\n",
    "        y_final = self.fc1(y_lstm[:, -1, :])\n",
    "        return y_final\n",
    "       \n",
    "class LstmDataset(Dataset):\n",
    "    def __init__(\n",
    "            self, \n",
    "            X_lstm: torch.Tensor,\n",
    "            y_lstm: torch.Tensor,\n",
    "    ) -> None:\n",
    "        self.X_lstm = X_lstm\n",
    "        self.y_lstm = y_lstm\n",
    "\n",
    "    def __len__(\n",
    "            self,\n",
    "    ) -> int:\n",
    "        return self.X_lstm.shape[0]\n",
    "        \n",
    "    def __getitem__(\n",
    "            self, \n",
    "            index\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        X = self.X_lstm[index,:,:]\n",
    "\n",
    "        y = self.y_lstm[index]\n",
    "        return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 10\n",
    "X_list = []\n",
    "y_list = []\n",
    "for i in range(df.shape[0]-L+1):\n",
    "    X_list.append(\n",
    "        df.iloc[i:i+L,:-1].loc[:,[\"feature1\", \"feature2\"]],\n",
    "    )\n",
    "    y_list.append(\n",
    "        df.iloc[i+L-1,-1],\n",
    "    )\n",
    "X_lstm = torch.Tensor(np.array(X_list)).float()\n",
    "y_lstm = torch.Tensor(np.array(y_list)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomas/.cache/pypoetry/virtualenvs/simplify-deployment-2FnGvFJr-py3.12/lib/python3.12/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9134, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4754, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4565, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8573, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0958, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8896, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9069, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7782, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9019, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5698, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomas/.cache/pypoetry/virtualenvs/simplify-deployment-2FnGvFJr-py3.12/lib/python3.12/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 done.\n",
      "tensor(1.1394, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4959, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7397, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0931, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7468, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6260, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1607, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8852, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2427, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7340, grad_fn=<MseLossBackward0>)\n",
      "Epoch 1 done.\n",
      "tensor(1.4114, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4853, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2539, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5803, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8410, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6334, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6425, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0057, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4151, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8137, grad_fn=<MseLossBackward0>)\n",
      "Epoch 2 done.\n",
      "tensor(0.9319, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6391, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2602, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6822, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8252, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0671, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1269, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1216, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9543, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2455, grad_fn=<MseLossBackward0>)\n",
      "Epoch 3 done.\n",
      "tensor(1.0224, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7233, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6209, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7452, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7861, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6136, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4487, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7800, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3246, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6714, grad_fn=<MseLossBackward0>)\n",
      "Epoch 4 done.\n",
      "tensor(0.9618, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2410, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5550, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8122, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5041, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8190, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2096, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4863, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4232, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0421, grad_fn=<MseLossBackward0>)\n",
      "Epoch 5 done.\n",
      "tensor(0.8820, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3218, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8466, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.7538, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6804, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7078, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1824, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8590, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7986, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.7195, grad_fn=<MseLossBackward0>)\n",
      "Epoch 6 done.\n",
      "tensor(0.8521, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9983, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0117, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.5085, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3939, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8307, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1627, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6161, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9158, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7859, grad_fn=<MseLossBackward0>)\n",
      "Epoch 7 done.\n",
      "tensor(0.6978, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1557, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6230, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5596, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8770, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9127, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.5719, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7709, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8427, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2636, grad_fn=<MseLossBackward0>)\n",
      "Epoch 8 done.\n",
      "tensor(1.0387, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.5025, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9845, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9139, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3665, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9570, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6816, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6100, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2329, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0709, grad_fn=<MseLossBackward0>)\n",
      "Epoch 9 done.\n",
      "tensor(2.0298, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1305, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3674, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8818, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7786, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4333, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7850, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1698, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9218, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9321, grad_fn=<MseLossBackward0>)\n",
      "Epoch 10 done.\n",
      "tensor(0.8331, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8644, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7888, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7116, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6564, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6913, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7052, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6361, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2822, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3002, grad_fn=<MseLossBackward0>)\n",
      "Epoch 11 done.\n",
      "tensor(1.0923, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0273, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0645, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4709, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6878, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.7728, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1424, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1155, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5134, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7832, grad_fn=<MseLossBackward0>)\n",
      "Epoch 12 done.\n",
      "tensor(0.5700, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3121, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.9657, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3717, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7665, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0768, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0929, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.7315, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4583, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1023, grad_fn=<MseLossBackward0>)\n",
      "Epoch 13 done.\n",
      "tensor(1.2172, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8702, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7650, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5259, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5074, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7302, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7055, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2999, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6094, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8799, grad_fn=<MseLossBackward0>)\n",
      "Epoch 14 done.\n",
      "tensor(1.1037, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5154, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8047, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8465, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0547, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8132, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9269, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9049, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4176, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8701, grad_fn=<MseLossBackward0>)\n",
      "Epoch 15 done.\n",
      "tensor(0.5396, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.9392, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8893, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2762, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7817, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7344, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4259, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.5475, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7415, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5229, grad_fn=<MseLossBackward0>)\n",
      "Epoch 16 done.\n",
      "tensor(0.5582, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4523, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9151, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0979, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0038, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6258, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1096, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3859, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9550, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2857, grad_fn=<MseLossBackward0>)\n",
      "Epoch 17 done.\n",
      "tensor(1.5199, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8913, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6072, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6910, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6430, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3704, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8288, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9803, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8930, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1353, grad_fn=<MseLossBackward0>)\n",
      "Epoch 18 done.\n",
      "tensor(0.3825, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4453, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1135, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6898, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8936, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5462, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.8572, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6141, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.7217, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1641, grad_fn=<MseLossBackward0>)\n",
      "Epoch 19 done.\n",
      "tensor(0.9341, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6833, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1315, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2979, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3964, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5536, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.5154, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4537, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9860, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9129, grad_fn=<MseLossBackward0>)\n",
      "Epoch 20 done.\n",
      "tensor(0.8028, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4133, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.9117, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8450, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1616, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8489, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1638, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2553, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6425, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8493, grad_fn=<MseLossBackward0>)\n",
      "Epoch 21 done.\n",
      "tensor(0.7341, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0954, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8962, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6464, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6745, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9004, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.5211, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9708, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1197, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.0799, grad_fn=<MseLossBackward0>)\n",
      "Epoch 22 done.\n",
      "tensor(0.8720, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2687, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1818, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3914, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9067, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3840, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4056, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0493, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8906, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8459, grad_fn=<MseLossBackward0>)\n",
      "Epoch 23 done.\n",
      "tensor(0.4134, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6051, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9549, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0033, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0104, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0398, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8342, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.5126, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8163, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.9012, grad_fn=<MseLossBackward0>)\n",
      "Epoch 24 done.\n",
      "tensor(1.5254, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7879, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5897, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4664, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6162, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4855, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0889, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1211, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9159, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2878, grad_fn=<MseLossBackward0>)\n",
      "Epoch 25 done.\n",
      "tensor(0.9920, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7665, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3918, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6285, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0651, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6878, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3428, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8606, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7358, grad_fn=<MseLossBackward0>)\n",
      "Epoch 26 done.\n",
      "tensor(1.2645, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5198, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.7250, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3612, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3404, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.5600, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5371, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4831, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4623, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3085, grad_fn=<MseLossBackward0>)\n",
      "Epoch 27 done.\n",
      "tensor(1.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8426, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.5460, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3576, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8301, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0341, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3005, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0104, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2468, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.5325, grad_fn=<MseLossBackward0>)\n",
      "Epoch 28 done.\n",
      "tensor(1.4613, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3318, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8887, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9860, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.1006, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2975, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6926, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4473, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9769, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9000, grad_fn=<MseLossBackward0>)\n",
      "Epoch 29 done.\n",
      "tensor(0.8593, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8077, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6396, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1056, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6831, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7733, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3833, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9917, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1890, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5142, grad_fn=<MseLossBackward0>)\n",
      "Epoch 30 done.\n",
      "tensor(1.6609, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8280, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2784, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4741, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8214, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0934, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4074, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5653, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1242, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2976, grad_fn=<MseLossBackward0>)\n",
      "Epoch 31 done.\n",
      "tensor(1.5845, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7293, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6679, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7250, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0040, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4427, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9113, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4900, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6384, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9636, grad_fn=<MseLossBackward0>)\n",
      "Epoch 32 done.\n",
      "tensor(0.7828, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3755, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3619, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7647, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9636, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1817, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0858, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3813, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7408, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8266, grad_fn=<MseLossBackward0>)\n",
      "Epoch 33 done.\n",
      "tensor(0.7358, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7339, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6876, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5076, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.0493, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2918, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9092, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5672, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8721, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8049, grad_fn=<MseLossBackward0>)\n",
      "Epoch 34 done.\n",
      "tensor(0.7996, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0131, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.7098, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4414, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2832, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4784, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6367, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1056, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9983, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2892, grad_fn=<MseLossBackward0>)\n",
      "Epoch 35 done.\n",
      "tensor(1.1326, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9630, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7442, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.5732, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8666, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1360, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5744, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7099, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4361, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3741, grad_fn=<MseLossBackward0>)\n",
      "Epoch 36 done.\n",
      "tensor(1.0902, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8116, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6577, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5087, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4246, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8259, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6284, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.8925, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9999, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6693, grad_fn=<MseLossBackward0>)\n",
      "Epoch 37 done.\n",
      "tensor(1.5146, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4914, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3030, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4446, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4258, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0159, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1673, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0438, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7933, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6418, grad_fn=<MseLossBackward0>)\n",
      "Epoch 38 done.\n",
      "tensor(0.8882, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4202, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2462, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0954, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6190, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9303, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2864, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4169, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2758, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8866, grad_fn=<MseLossBackward0>)\n",
      "Epoch 39 done.\n",
      "tensor(2.0113, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4621, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9118, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6207, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0557, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6184, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6974, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7604, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4243, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3188, grad_fn=<MseLossBackward0>)\n",
      "Epoch 40 done.\n",
      "tensor(0.4597, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9648, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9362, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.5327, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0029, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.7683, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1106, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0211, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0057, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7937, grad_fn=<MseLossBackward0>)\n",
      "Epoch 41 done.\n",
      "tensor(0.9999, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6031, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6359, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7594, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6980, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5550, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4903, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0006, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3310, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0950, grad_fn=<MseLossBackward0>)\n",
      "Epoch 42 done.\n",
      "tensor(1.3103, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4647, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8184, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7708, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.5662, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8497, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1661, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3497, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5462, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0083, grad_fn=<MseLossBackward0>)\n",
      "Epoch 43 done.\n",
      "tensor(1.0668, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6734, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2973, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5538, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9194, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9040, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2371, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1183, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7218, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1059, grad_fn=<MseLossBackward0>)\n",
      "Epoch 44 done.\n",
      "tensor(0.6488, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2334, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2961, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2424, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3143, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4633, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5786, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7669, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0286, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6568, grad_fn=<MseLossBackward0>)\n",
      "Epoch 45 done.\n",
      "tensor(1.6589, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6019, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3426, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3339, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.5815, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8130, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4407, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4931, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8370, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9705, grad_fn=<MseLossBackward0>)\n",
      "Epoch 46 done.\n",
      "tensor(0.9264, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1665, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0015, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3459, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4190, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7120, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6237, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8144, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.2759, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6937, grad_fn=<MseLossBackward0>)\n",
      "Epoch 47 done.\n",
      "tensor(1.2919, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5059, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1307, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8117, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0720, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9258, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4110, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6841, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6738, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2582, grad_fn=<MseLossBackward0>)\n",
      "Epoch 48 done.\n",
      "tensor(0.8548, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1314, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4842, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3440, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9543, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2253, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7394, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3397, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.7627, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.5030, grad_fn=<MseLossBackward0>)\n",
      "Epoch 49 done.\n",
      "tensor(1.0131, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9233, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9219, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2396, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2183, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3787, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7180, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5449, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4842, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6547, grad_fn=<MseLossBackward0>)\n",
      "Epoch 50 done.\n",
      "tensor(0.9097, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1523, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8319, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.0592, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9308, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1779, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1723, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6228, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9703, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8830, grad_fn=<MseLossBackward0>)\n",
      "Epoch 51 done.\n",
      "tensor(1.0346, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5817, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8615, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7376, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0120, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4764, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2101, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3979, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4371, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5688, grad_fn=<MseLossBackward0>)\n",
      "Epoch 52 done.\n",
      "tensor(1.0842, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5706, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9805, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1841, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9357, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2090, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6671, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0777, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4558, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8196, grad_fn=<MseLossBackward0>)\n",
      "Epoch 53 done.\n",
      "tensor(1.0913, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8547, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.8554, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7584, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1974, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3118, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7589, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7907, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6914, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2399, grad_fn=<MseLossBackward0>)\n",
      "Epoch 54 done.\n",
      "tensor(0.8686, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4894, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4139, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0479, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4189, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7843, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0250, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9509, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8349, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7954, grad_fn=<MseLossBackward0>)\n",
      "Epoch 55 done.\n",
      "tensor(0.5560, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7186, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2131, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4612, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7860, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4252, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9690, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9215, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1703, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9183, grad_fn=<MseLossBackward0>)\n",
      "Epoch 56 done.\n",
      "tensor(0.7960, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1288, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6142, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9247, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2498, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3870, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0011, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.7055, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0227, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3592, grad_fn=<MseLossBackward0>)\n",
      "Epoch 57 done.\n",
      "tensor(0.2384, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4400, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1488, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1767, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4702, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6195, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9927, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8052, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8667, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3874, grad_fn=<MseLossBackward0>)\n",
      "Epoch 58 done.\n",
      "tensor(0.7085, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8694, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.9801, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9438, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0514, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8585, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4337, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.5149, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5725, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2497, grad_fn=<MseLossBackward0>)\n",
      "Epoch 59 done.\n",
      "tensor(1.0526, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6706, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.9027, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6999, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6519, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9558, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0157, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.8055, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.5850, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0283, grad_fn=<MseLossBackward0>)\n",
      "Epoch 60 done.\n",
      "tensor(0.8960, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.1270, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8538, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5925, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9180, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8286, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0116, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1723, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6204, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8185, grad_fn=<MseLossBackward0>)\n",
      "Epoch 61 done.\n",
      "tensor(0.8671, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6720, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7054, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.0331, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2581, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5422, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5692, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9325, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2151, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7171, grad_fn=<MseLossBackward0>)\n",
      "Epoch 62 done.\n",
      "tensor(0.7269, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1732, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6860, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7563, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0713, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9098, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6437, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2596, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8017, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6444, grad_fn=<MseLossBackward0>)\n",
      "Epoch 63 done.\n",
      "tensor(1.2106, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4342, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1406, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0828, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6563, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8905, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8845, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1263, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3882, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9090, grad_fn=<MseLossBackward0>)\n",
      "Epoch 64 done.\n",
      "tensor(1.2269, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9554, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3166, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2454, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4136, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3245, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1425, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7189, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4981, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7459, grad_fn=<MseLossBackward0>)\n",
      "Epoch 65 done.\n",
      "tensor(1.3056, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3328, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4907, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2284, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6980, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0567, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0341, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8138, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.5043, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6622, grad_fn=<MseLossBackward0>)\n",
      "Epoch 66 done.\n",
      "tensor(1.1644, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8449, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0100, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.9636, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4969, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0222, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6944, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6018, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9557, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5498, grad_fn=<MseLossBackward0>)\n",
      "Epoch 67 done.\n",
      "tensor(0.9574, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0365, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4845, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0051, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7285, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4953, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5593, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9627, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5980, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4480, grad_fn=<MseLossBackward0>)\n",
      "Epoch 68 done.\n",
      "tensor(1.0838, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2123, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5759, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6998, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9268, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2604, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2411, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6603, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2913, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6762, grad_fn=<MseLossBackward0>)\n",
      "Epoch 69 done.\n",
      "tensor(0.6480, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2494, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8454, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8661, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8217, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7942, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0256, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.9060, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9170, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2883, grad_fn=<MseLossBackward0>)\n",
      "Epoch 70 done.\n",
      "tensor(1.2927, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8897, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8247, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3196, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6481, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8760, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5233, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5866, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2341, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3225, grad_fn=<MseLossBackward0>)\n",
      "Epoch 71 done.\n",
      "tensor(0.7829, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3464, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8374, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5172, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1702, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6148, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5929, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5799, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4150, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2211, grad_fn=<MseLossBackward0>)\n",
      "Epoch 72 done.\n",
      "tensor(1.0744, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9848, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9061, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8666, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.5391, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8207, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.9141, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6824, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0401, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4732, grad_fn=<MseLossBackward0>)\n",
      "Epoch 73 done.\n",
      "tensor(1.4856, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1073, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0485, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7889, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7267, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0733, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9692, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8203, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3528, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4150, grad_fn=<MseLossBackward0>)\n",
      "Epoch 74 done.\n",
      "tensor(1.1861, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3015, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8237, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8665, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6732, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0354, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6220, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9805, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9056, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0410, grad_fn=<MseLossBackward0>)\n",
      "Epoch 75 done.\n",
      "tensor(1.5563, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1989, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5252, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1785, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6045, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9506, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4905, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0722, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3192, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0552, grad_fn=<MseLossBackward0>)\n",
      "Epoch 76 done.\n",
      "tensor(0.7951, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5994, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6514, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6508, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9295, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0237, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4997, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7441, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3516, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6483, grad_fn=<MseLossBackward0>)\n",
      "Epoch 77 done.\n",
      "tensor(0.5699, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9969, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3289, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6196, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6700, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4525, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8698, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4603, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9228, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0492, grad_fn=<MseLossBackward0>)\n",
      "Epoch 78 done.\n",
      "tensor(1.5193, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0693, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4015, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2351, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4290, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.9087, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4849, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4006, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6983, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9687, grad_fn=<MseLossBackward0>)\n",
      "Epoch 79 done.\n",
      "tensor(0.7833, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7995, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0610, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8387, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7272, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8952, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1985, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9883, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6496, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8305, grad_fn=<MseLossBackward0>)\n",
      "Epoch 80 done.\n",
      "tensor(1.2358, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.5234, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6009, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6936, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7446, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9023, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5132, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2130, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3864, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.7200, grad_fn=<MseLossBackward0>)\n",
      "Epoch 81 done.\n",
      "tensor(1.0210, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.1111, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0317, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0367, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0789, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0877, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6176, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0589, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8078, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.5819, grad_fn=<MseLossBackward0>)\n",
      "Epoch 82 done.\n",
      "tensor(0.4726, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0035, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.8973, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2411, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9837, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6868, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0834, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6720, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2611, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1438, grad_fn=<MseLossBackward0>)\n",
      "Epoch 83 done.\n",
      "tensor(1.2517, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8973, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.5368, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8681, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0205, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9587, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7720, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1184, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7597, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6893, grad_fn=<MseLossBackward0>)\n",
      "Epoch 84 done.\n",
      "tensor(0.9151, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1139, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1245, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3097, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8422, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4210, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8798, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1132, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3049, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5527, grad_fn=<MseLossBackward0>)\n",
      "Epoch 85 done.\n",
      "tensor(1.0074, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1091, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2066, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9510, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0355, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1433, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3983, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6868, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0106, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1940, grad_fn=<MseLossBackward0>)\n",
      "Epoch 86 done.\n",
      "tensor(0.9772, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8242, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1922, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5685, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6446, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7770, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.9236, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0098, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8675, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1882, grad_fn=<MseLossBackward0>)\n",
      "Epoch 87 done.\n",
      "tensor(1.1213, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7753, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1744, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8554, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3359, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7406, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2629, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4203, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8918, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1045, grad_fn=<MseLossBackward0>)\n",
      "Epoch 88 done.\n",
      "tensor(1.0933, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.5093, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8893, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2187, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5806, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9706, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4974, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8617, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0420, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7838, grad_fn=<MseLossBackward0>)\n",
      "Epoch 89 done.\n",
      "tensor(0.3691, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1021, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7597, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6706, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6548, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8673, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3661, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3386, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.1030, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3585, grad_fn=<MseLossBackward0>)\n",
      "Epoch 90 done.\n",
      "tensor(1.0765, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8725, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2074, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.5615, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4294, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.7787, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4128, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9329, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6395, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1296, grad_fn=<MseLossBackward0>)\n",
      "Epoch 91 done.\n",
      "tensor(0.5850, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8120, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0483, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8156, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6013, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4144, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4208, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1543, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4407, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.0313, grad_fn=<MseLossBackward0>)\n",
      "Epoch 92 done.\n",
      "tensor(1.4618, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6950, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0733, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8267, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6873, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4533, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2160, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0403, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4242, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4384, grad_fn=<MseLossBackward0>)\n",
      "Epoch 93 done.\n",
      "tensor(0.5210, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6058, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3843, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6552, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5696, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5972, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5775, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7440, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8716, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5090, grad_fn=<MseLossBackward0>)\n",
      "Epoch 94 done.\n",
      "tensor(0.4619, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2896, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4658, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4550, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4933, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5295, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9848, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8733, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.2012, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8763, grad_fn=<MseLossBackward0>)\n",
      "Epoch 95 done.\n",
      "tensor(1.4134, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6864, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9091, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9016, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6851, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5193, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3159, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7558, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1947, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7281, grad_fn=<MseLossBackward0>)\n",
      "Epoch 96 done.\n",
      "tensor(1.4205, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5982, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4668, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0529, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3462, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5555, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4116, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0990, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9280, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2572, grad_fn=<MseLossBackward0>)\n",
      "Epoch 97 done.\n",
      "tensor(1.2708, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4825, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7630, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0375, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2343, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8531, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3145, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.7155, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9390, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1280, grad_fn=<MseLossBackward0>)\n",
      "Epoch 98 done.\n",
      "tensor(1.0122, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9944, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.6714, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7556, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0275, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1764, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1730, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.5385, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0636, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6993, grad_fn=<MseLossBackward0>)\n",
      "Epoch 99 done.\n"
     ]
    }
   ],
   "source": [
    "lstm_epochs = 100\n",
    "lstm_plt_dict = {\n",
    "    \"x\": [],\n",
    "    \"y\": [],\n",
    "}\n",
    "lstm_dataset = LstmDataset(\n",
    "    X_lstm=X_lstm,\n",
    "    y_lstm=y_lstm,\n",
    ")\n",
    "\n",
    "lstm_dataloader = DataLoader(\n",
    "    lstm_dataset,\n",
    "    batch_size=10,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "lstm_model = LstmModel()\n",
    "lstm_criterion = torch.nn.MSELoss()\n",
    "\n",
    "lstm_optimizer = Adam(\n",
    "    params = lstm_model.parameters(),\n",
    "    lr = 1/1000,\n",
    "    weight_decay=1e-5,\n",
    "\n",
    ")\n",
    "for epoch in range(lstm_epochs):\n",
    "    for i,(lstm_X, lstm_y_true) in enumerate(lstm_dataloader):\n",
    "        lstm_optimizer.zero_grad()\n",
    "        lstm_y_pred = lstm_model(lstm_X)\n",
    "        lstm_loss = lstm_criterion(\n",
    "            lstm_y_pred,\n",
    "            lstm_y_true,\n",
    "        )\n",
    "        lstm_loss.backward()\n",
    "        lstm_optimizer.step()\n",
    "        if i % 100 == 0:\n",
    "            print(lstm_loss)\n",
    "        lstm_plt_dict[\"x\"].append(i*(epoch+1))\n",
    "        lstm_plt_dict[\"y\"].append(lstm_loss.item())\n",
    "        \n",
    "    print(f\"Epoch {epoch} done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lstm_fig, lstm_ax = plt.subplots()\n",
    "lstm_ax.scatter(lstm_plt_dict[\"x\"],lstm_plt_dict[\"y\"])\n",
    "lstm_ax.set_xlabel(\"iteration\")\n",
    "lstm_ax.set_ylabel(\"loss\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_trainable_parameters(model: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "count_trainable_parameters(lstm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor().numel()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simplify-deployment-2FnGvFJr-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
