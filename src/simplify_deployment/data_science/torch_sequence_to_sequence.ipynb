{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/200], Loss: 0.1590\n",
      "Epoch [40/200], Loss: 0.0863\n",
      "Epoch [60/200], Loss: 0.0843\n",
      "Epoch [80/200], Loss: 0.0837\n",
      "Epoch [100/200], Loss: 0.0836\n",
      "Epoch [120/200], Loss: 0.0836\n",
      "Epoch [140/200], Loss: 0.0835\n",
      "Epoch [160/200], Loss: 0.0835\n",
      "Epoch [180/200], Loss: 0.0835\n",
      "Epoch [200/200], Loss: 0.0835\n",
      "tensor([[[0.5061, 0.5063, 0.5029],\n",
      "         [0.5068, 0.4998, 0.5053],\n",
      "         [0.5016, 0.4957, 0.5044],\n",
      "         [0.4954, 0.4916, 0.5009],\n",
      "         [0.4898, 0.4883, 0.4961]]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "n_steps_in = 10  # Number of input steps\n",
    "n_steps_out = 5  # Number of output steps\n",
    "n_features = 2   # Number of input features\n",
    "hidden_size = 50 # Number of LSTM units\n",
    "num_epochs = 200 # Number of training epochs\n",
    "output_features = 3  # Number of output features\n",
    "\n",
    "# Generate dummy data\n",
    "X = np.random.rand(1000, n_steps_in, n_features).astype(np.float32)\n",
    "y = np.random.rand(1000, n_steps_out, output_features).astype(np.float32)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X = torch.from_numpy(X)\n",
    "y = torch.from_numpy(y)\n",
    "\n",
    "# Define the Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, (h, c) = self.lstm(x)\n",
    "        return h, c\n",
    "\n",
    "# Define the Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, num_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(output_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x, h, c):\n",
    "        out, (h, c) = self.lstm(x, (h, c))\n",
    "        out = self.fc(out)\n",
    "        return out, h, c\n",
    "\n",
    "# Define the Seq2Seq model\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, n_steps_out, n_features, output_features):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.n_steps_out = n_steps_out\n",
    "        self.n_features = n_features\n",
    "        self.output_features = output_features\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h, c = self.encoder(x)\n",
    "        decoder_input = torch.zeros((x.size(0), 1, self.output_features)).to(x.device)\n",
    "        outputs = []\n",
    "        for _ in range(self.n_steps_out):\n",
    "            out, h, c = self.decoder(decoder_input, h, c)\n",
    "            outputs.append(out)\n",
    "            decoder_input = out\n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        return outputs\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "encoder = Encoder(n_features, hidden_size)\n",
    "decoder = Decoder(hidden_size, output_features)\n",
    "model = Seq2Seq(encoder, decoder, n_steps_out, n_features, output_features)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X)\n",
    "    loss = criterion(output, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Predict\n",
    "model.eval()\n",
    "X_new = torch.from_numpy(np.random.rand(1, n_steps_in, n_features).astype(np.float32))\n",
    "y_pred = model(X_new)\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "out,_ = encoder(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 10, 2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000, 50])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simplify-deployment-2FnGvFJr-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
