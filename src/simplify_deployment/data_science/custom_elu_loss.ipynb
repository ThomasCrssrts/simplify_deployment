{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn \n",
    "from sklearn.datasets import make_regression\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from typing import Tuple\n",
    "from sklearn.linear_model import LinearRegression, HuberRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import plotly.express as px\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import root_mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 0\n",
    "data_folder = Path(\n",
    "    \"/home/thomas/repos/simplify_deployment/data/data_science/50_gen_s1/folds_converted_with_genome\",\n",
    ")\n",
    "X_train = pd.read_parquet(data_folder/f\"X_train_fold_{fold}.parquet\")\n",
    "y_train = pd.read_parquet(data_folder/f\"y_train_fold_{fold}.parquet\").squeeze()\n",
    "X_test = pd.read_parquet(data_folder/f\"X_test_fold_{fold}.parquet\")\n",
    "y_test = pd.read_parquet(data_folder/f\"y_test_fold_{fold}.parquet\").squeeze()\n",
    "n_features = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "s1 model for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_folder = Path(\n",
    "    \"/home/thomas/repos/simplify_deployment/data/simplify_1_0/folds_converted_with_genome\",\n",
    ")\n",
    "X_train_s1 = pd.read_parquet(s1_folder/f\"X_train_fold_{fold}.parquet\")\n",
    "y_train_s1 = pd.read_parquet(s1_folder/f\"y_train_fold_{fold}.parquet\").squeeze()\n",
    "X_test_s1 = pd.read_parquet(s1_folder/f\"X_test_fold_{fold}.parquet\")\n",
    "y_test_s1 = pd.read_parquet(s1_folder/f\"y_test_fold_{fold}.parquet\").squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_model = LinearRegression()\n",
    "s1_model.fit(X_train_s1, y_train_s1)\n",
    "s1_prediction = pd.DataFrame(\n",
    "    {\n",
    "        \"y_true\": y_test_s1,\n",
    "        \"y_pred_s1\": s1_model.predict(X_test_s1)\n",
    "    },\n",
    "    index = y_test_s1.index\n",
    ")\n",
    "s1_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard linear regression\n",
    "Let's start by fitting a standard linear regression and plotting the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_model = LinearRegression()\n",
    "standard_model.fit(X_train, y_train)\n",
    "prediction_df = pd.DataFrame(\n",
    "    {\n",
    "        \"y_true\": y_test,\n",
    "        \"y_pred_standard_model\": standard_model.predict(X_test),\n",
    "    },\n",
    "    index = y_test.index\n",
    ")\n",
    "prediction_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom loss model\n",
    "This time we will fit a linear model with our custom loss function. It should perform less well in regards to RMSE as this it not what it optimizes for.\n",
    "We should see less max errors, or less time above threshold, ... depending on what we prioritize in the loss function.\n",
    "\n",
    "For the optimizer to work well, it is best to standardscale the data. This brings some added complexity in the code, but is necessary to speed up convergence.\n",
    "\n",
    "Let's starts by setting up some classes we will need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_features: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(\n",
    "            in_features=n_features,\n",
    "            out_features=1,\n",
    "        )  # Just 1 fully connected layer without activation, i.e. a linear regression.\n",
    "\n",
    "    def forward(\n",
    "        self,  \n",
    "        X: torch.Tensor,  \n",
    "    ) -> torch.Tensor:\n",
    "        y = self.fc1(X)\n",
    "        return y.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            threshold: float = 0.685,\n",
    "            weight_max_error: float = 1,\n",
    "            weight_percentage_above_threshold:float = 1,\n",
    "            weight_wrong_sign: float = 1,\n",
    "            sigmoid_steepness: float = 1,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.steepness = sigmoid_steepness\n",
    "        self.threshold = threshold\n",
    "        # Normalize weights and assign them\n",
    "        sum_weights = (\n",
    "            weight_max_error\n",
    "            + weight_percentage_above_threshold\n",
    "            + weight_wrong_sign\n",
    "        )\n",
    "        self.weight_max_error = (\n",
    "            weight_max_error / sum_weights\n",
    "        )\n",
    "        self.weight_percentage_above_threshold = (\n",
    "            weight_percentage_above_threshold / sum_weights\n",
    "        )\n",
    "        self.weight_wrong_sign = (\n",
    "            weight_wrong_sign / sum_weights\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            inputs: torch.Tensor, \n",
    "            targets: torch.Tensor,\n",
    "        ) -> torch.Tensor:\n",
    "\n",
    "        residuals = targets - inputs\n",
    "        # Maximum abs error\n",
    "        max_error = residuals.abs().max()\n",
    "\n",
    "        # Percentage of time above threshold value\n",
    "        percentage_of_time_above_x = (\n",
    "            1/(1+torch.e**(-self.steepness*(residuals.abs()-self.threshold)))\n",
    "        ).mean()\n",
    "\n",
    "        # Percentage of time wrong sign\n",
    "        loss_percentage_of_time_wrong_sign = (\n",
    "            1/(1+torch.e**(-self.steepness*(inputs*targets)))\n",
    "        ).mean()\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = (\n",
    "            self.weight_max_error * max_error\n",
    "            + self.weight_percentage_above_threshold * percentage_of_time_above_x\n",
    "            + self.weight_wrong_sign * loss_percentage_of_time_wrong_sign\n",
    "        )\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLossRelu(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            threshold: float = 0.685,\n",
    "            weight_max_error: float = 1,\n",
    "            weight_percentage_above_threshold:float = 1,\n",
    "            weight_wrong_sign: float = 1,\n",
    "            sigmoid_steepness: float = 1,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "        # Normalize weights and assign them\n",
    "        sum_weights = (\n",
    "            weight_max_error\n",
    "            + weight_percentage_above_threshold\n",
    "            + weight_wrong_sign\n",
    "        )\n",
    "        self.weight_max_error = (\n",
    "            weight_max_error / sum_weights\n",
    "        )\n",
    "        self.weight_percentage_above_threshold = (\n",
    "            weight_percentage_above_threshold / sum_weights\n",
    "        )\n",
    "        self.weight_wrong_sign = (\n",
    "            weight_wrong_sign / sum_weights\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            inputs: torch.Tensor, \n",
    "            targets: torch.Tensor,\n",
    "        ) -> torch.Tensor:\n",
    "\n",
    "        residuals = targets - inputs\n",
    "        # Maximum abs error\n",
    "        max_error = residuals.abs().max()\n",
    "\n",
    "        # Percentage of time above threshold value\n",
    "        percentage_of_time_above_x = nn.functional.relu(\n",
    "            residuals.abs()-self.threshold\n",
    "        ).mean()\n",
    "\n",
    "        # Percentage of time wrong sign\n",
    "        loss_percentage_of_time_wrong_sign = nn.functional.relu(\n",
    "            -targets*inputs\n",
    "        ).mean()\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = (\n",
    "            self.weight_max_error * max_error\n",
    "            + self.weight_percentage_above_threshold * percentage_of_time_above_x\n",
    "            + self.weight_wrong_sign * loss_percentage_of_time_wrong_sign\n",
    "        )\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMSELoss(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            inputs: torch.Tensor, \n",
    "            targets: torch.Tensor,\n",
    "        ) -> torch.Tensor:\n",
    "\n",
    "        residuals = targets - inputs\n",
    "        mse = (residuals**2).mean()\n",
    "        return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            X: torch.Tensor,\n",
    "            y: torch.Tensor,\n",
    "        ) -> None:\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(\n",
    "            self\n",
    "    ) -> int:\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(\n",
    "            self,\n",
    "            idx: int,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        X_item = self.X[idx,:]\n",
    "        y_item = self.y[idx]\n",
    "        return X_item, y_item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaler = StandardScaler().fit(X_train)\n",
    "y_scaler = StandardScaler().fit(y_train.to_frame())\n",
    "\n",
    "X_train_scaled = torch.Tensor(\n",
    "    X_scaler.transform(X_train),\n",
    ").float()\n",
    "\n",
    "X_test_scaled = torch.Tensor(\n",
    "    X_scaler.transform(X_test),\n",
    ").float()\n",
    "\n",
    "y_train_scaled = torch.Tensor(\n",
    "    y_scaler.transform(y_train.to_frame()).squeeze(), # Make target tensor unidimensional\n",
    ").float()\n",
    "\n",
    "y_test_scaled = torch.Tensor(\n",
    "    y_scaler.transform(y_test.to_frame()).squeeze(),\n",
    ").float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom loss model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "lr = 1e-4\n",
    "batch_size = 96\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    CustomDataset(X_train_scaled,y_train_scaled),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "model = Model(\n",
    "    n_features=n_features,\n",
    ")\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr = lr,\n",
    ")\n",
    "threshold = 117\n",
    "converted_threshold = threshold / np.std(y_train)\n",
    "criterion = CustomLossRelu(\n",
    "    threshold=converted_threshold,\n",
    "    weight_wrong_sign=1,\n",
    "    weight_max_error=1,\n",
    "    weight_percentage_above_threshold=1,\n",
    ") \n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for X_batch, y_batch in dataloader:\n",
    "        prediction = model(X_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(prediction, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    average_loss = epoch_loss/len(dataloader)\n",
    "    print(f\"Average epoch loss: {average_loss}\")\n",
    "    print(f\"Epoch {epoch} done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df[\"y_pred_custom_loss\"] = y_scaler.inverse_transform(\n",
    "    model(X_test_scaled).detach().numpy().reshape(-1,1),\n",
    ")\n",
    "prediction_df = prediction_df.merge(\n",
    "    s1_prediction.drop(columns = \"y_true\"), \n",
    "    left_index = True, \n",
    "    right_index=True, \n",
    "    how = \"inner\",\n",
    ")\n",
    "prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_rmse = root_mean_squared_error(\n",
    "    prediction_df[\"y_true\"],\n",
    "    prediction_df[\"y_pred_s1\"],\n",
    ")\n",
    "print(f\"Rmse of s1 is {s1_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_rmse = root_mean_squared_error(\n",
    "    prediction_df[\"y_true\"],\n",
    "    prediction_df[\"y_pred_standard_model\"],\n",
    ")\n",
    "print(f\"Rmse of standard model is {standard_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_rmse = root_mean_squared_error(\n",
    "    prediction_df[\"y_true\"],\n",
    "    prediction_df[\"y_pred_custom_loss\"],\n",
    ")\n",
    "print(f\"Rmse of custom model is {custom_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df_molten = prediction_df.melt(ignore_index=False)\n",
    "fig = px.line(\n",
    "    prediction_df_molten,\n",
    "    x = prediction_df_molten.index,\n",
    "    y = \"value\",\n",
    "    color = \"variable\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simplify-deployment-2FnGvFJr-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
